{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db452cd-4a3b-4690-8d27-cd30e88f005a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "import torchvision.models as models\n",
    "from collections import OrderedDict\n",
    "from torch.autograd import Variable\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "import pickle\n",
    "import pandas\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as st\n",
    "import gc\n",
    "import random\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import tqdm\n",
    "import cv2\n",
    "from typing import Callable, List, Optional, Tuple\n",
    "import torchvision.transforms as transforms\n",
    "import ttach as tta\n",
    "\n",
    "\n",
    "from torchvision import transforms\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49159cfe-b1bf-4735-b34d-fa3d4a4f044d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9db6be1-b4ad-49dc-b6b5-995aa84bec2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
    "#A) main functions definition\n",
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
    "\n",
    "class ResNet_18(nn.Module):\n",
    "    def __init__(self, output_classes=3):\n",
    "        super(ResNet_18, self).__init__()\n",
    "        self.resnet_model = models.resnet18(pretrained=True)\n",
    "        self.resnet_model.fc = nn.Sequential(OrderedDict([\n",
    "            ('fc1', nn.Linear(512, 128)), ('relu', nn.ReLU()),\n",
    "            ('fc2', nn.Linear(128, output_classes))\n",
    "        ]))\n",
    "    \n",
    "    def forward(self, x, apply_sigmoid=False):\n",
    "        logits = self.resnet_model(x)\n",
    "        if apply_sigmoid:\n",
    "            return torch.sigmoid(logits)\n",
    "        return logits\n",
    "\n",
    "def fit_model(model, X_data, y_data, EPOCHS=5, BATCH_SIZE=32):\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    error = nn.BCEWithLogitsLoss()  \n",
    "\n",
    "    model.train()\n",
    "    n = X_data.shape[0]\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        obsIDs = np.arange(X_data.shape[0])\n",
    "        np.random.shuffle(obsIDs)\n",
    "        \n",
    "        epoch_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        for batch_start in range(0, n, BATCH_SIZE):\n",
    "            if batch_start + BATCH_SIZE > n:\n",
    "                break\n",
    "\n",
    "            Curr_obsIDs = obsIDs[batch_start:batch_start + BATCH_SIZE]\n",
    "            var_X_batch = X_data[Curr_obsIDs,:,:,:].float().to(DEVICE)\n",
    "            var_y_batch = y_data[Curr_obsIDs,:].float().to(DEVICE)  \n",
    "            optimizer.zero_grad()\n",
    "            output = model(var_X_batch)  \n",
    "            \n",
    "            loss = error(output, var_y_batch)  \n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            probabilities = torch.sigmoid(output)  \n",
    "            predictions = (probabilities > 0.5).float()  \n",
    "            \n",
    "            correct_predictions += (predictions == var_y_batch).sum().item()\n",
    "            total_samples += var_y_batch.numel()  \n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        epoch_accuracy = (correct_predictions / total_samples) * 100  \n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{EPOCHS}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%')\n",
    "\n",
    "    torch.save(model.state_dict(), \"./modelFinal.pytorchModel\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def LargeDatasetPred(model, var_X, BlockSizes, DEVICE='cpu'):\n",
    "    \"\"\"\n",
    "    Prediction of large datasets (incrementally predicts with 'model' blocks of observations out of 'var_X' having a size of 'BlockSizes').\n",
    "    To be used in case the RAM is not large enough to treat all information in the NN.\n",
    "    \"\"\"\n",
    "    n_loc = var_X.shape[0]\n",
    "\n",
    "    loc_miniBatch_Start = 0\n",
    "\n",
    "    while loc_miniBatch_Start < n_loc:\n",
    "        # Define the mini-batch domain\n",
    "        loc_miniBatch_End = loc_miniBatch_Start + BlockSizes\n",
    "        if loc_miniBatch_End >= n_loc:\n",
    "            loc_miniBatch_End = n_loc\n",
    "\n",
    "        # Local prediction\n",
    "        with torch.no_grad():\n",
    "            loc_predY = model(var_X[loc_miniBatch_Start:loc_miniBatch_End,:,:,:].to(DEVICE)).to('cpu')\n",
    "\n",
    "        # Merge local prediction with former ones\n",
    "        if loc_miniBatch_Start == 0:\n",
    "            all_predY = torch.clone(loc_predY)\n",
    "        else:\n",
    "            all_predY = torch.cat([all_predY, loc_predY], dim=0)\n",
    "\n",
    "        # Increment loc_miniBatch_Start\n",
    "        loc_miniBatch_Start += BlockSizes\n",
    "\n",
    "    return all_predY\n",
    "    \n",
    "\n",
    "def showRGBImage(LodID,X,Y,S,M):\n",
    "  \"\"\"\n",
    "  show observations\n",
    "  \"\"\"\n",
    "  LocImage=(X[LodID,:,:,:]*255).astype(int)\n",
    "  LocTitle='Y='+str(int(Y[LodID,0]))+'    E='+str(int(S[LodID,0])) +  '     S='+str(int(M[LodID,0]))\n",
    "  plt.figure() \n",
    "  plt.imshow(LocImage)\n",
    "  plt.title(LocTitle)\n",
    "  plt.show()\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e71b82a-e94f-4dfc-9be3-a529ed4b6d41",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Info_all_train = pd.read_csv('./DATA_celebA/train.csv')\n",
    "\n",
    "Y = Info_all_train.astype(np.float32)\n",
    "\n",
    "print(np.shape(Y))\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr_matrix = Y.corr()\n",
    "\n",
    "# Visualize the correlation matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, cmap='coolwarm', linewidths=.05)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423ca381-b4d9-48f3-a266-8768a7d1fc44",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Flatten the correlation matrix with stack and reset index\n",
    "corr_flat = corr_matrix.stack().reset_index()\n",
    "corr_flat.columns = ['Variable1', 'Variable2', 'Correlation']\n",
    "\n",
    "# Remove self-correlation and duplicate pairs\n",
    "corr_flat = corr_flat[corr_flat['Variable1'] != corr_flat['Variable2']]\n",
    "corr_flat['abs_correlation'] = corr_flat['Correlation'].abs()\n",
    "corr_flat = corr_flat.drop_duplicates(subset=['abs_correlation'])\n",
    "\n",
    "# Get the top 10 most correlated pairs\n",
    "top_correlations = corr_flat.sort_values(by='abs_correlation', ascending=False).head(100)\n",
    "\n",
    "print(top_correlations[['Variable1', 'Variable2', 'Correlation']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136f3182-15b7-40c8-9a2b-4b3e0904dec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Info_all_train.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8014388d-6747-4b86-b685-c00cd2f6c36c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training set\n",
    "with open('./DATA_celebA/train_64x64.pkl', 'rb') as infile:\n",
    "    X_train = pickle.load(infile)\n",
    "\n",
    "Info_all_train = pd.read_csv('./DATA_celebA/train.csv')\n",
    "\n",
    "#labels = ['Young', 'Eyeglasses', 'Smiling']\n",
    "\n",
    "labels = Info_all_train.columns.tolist()\n",
    "l = len(labels)\n",
    "Y_train = Info_all_train[labels].values.astype(np.float32).reshape(-1, l)\n",
    "\n",
    "#Test set\n",
    "with open('./DATA_celebA/test_64x64.pkl', 'rb') as infile:\n",
    "    X_test = pickle.load(infile)\n",
    "\n",
    "Info_all_test = pd.read_csv('./DATA_celebA/test.csv')\n",
    "Y_test = Info_all_test[labels].values.astype(np.float32).reshape(-1, l)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7df7bd5-7bbd-4248-897f-4933d866bbd6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get 20 random indices\n",
    "random_indices = random.sample(range(X_train.shape[0]), 50)\n",
    "\n",
    "# Save images without labels\n",
    "for idx in random_indices:\n",
    "    img = X_train[idx]  # Get the image\n",
    "    img = (img * 255).astype(np.uint8)  # Scale the image back to [0, 255] if needed\n",
    "    img_filename = f'image_{idx}.png'  # Create a filename for saving\n",
    "    cv2.imwrite(img_filename, cv2.cvtColor(img, cv2.COLOR_RGB2BGR))  # Save the image\n",
    "    print(idx)\n",
    "    showRGBImage(idx, X_train, Y_train[:,0].reshape(-1,1) , Y_train[:,1].reshape(-1,1), Y_train[:,2].reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e46c01-54ac-424f-a824-1f9429f4fec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to torch format\n",
    "torch_X_train = torch.from_numpy(X_train[:, :, :, :]).type(torch.FloatTensor).transpose(1, 3).to(DEVICE)\n",
    "torch_y_train = torch.from_numpy(Y_train).type(torch.FloatTensor).to(DEVICE)\n",
    "\n",
    "torch_X_test = torch.from_numpy(X_test[:, :, :, :]).type(torch.FloatTensor).transpose(1, 3).to(DEVICE)\n",
    "torch_y_test = torch.from_numpy(Y_test).type(torch.FloatTensor).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b30e88-5cb0-4bc5-a335-0d7e3d78d375",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# C) Training phase\n",
    "model = ResNet_18(l).to(DEVICE)\n",
    "EPOCHS_in = 50\n",
    "BATCH_SIZE_in = 512\n",
    "fit_model(model, torch_X_train, torch_y_train, EPOCHS=EPOCHS_in, BATCH_SIZE=BATCH_SIZE_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f7b231-4c7a-4423-ab2f-ca032a73c33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test = ResNet_18(l)  \n",
    "model_test.load_state_dict(torch.load(\"./modelFinal.pytorchModel\"))\n",
    "model_test.to(DEVICE)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11270fe-d069-4701-bed7-fe0e8c492ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test.eval()\n",
    "\n",
    "#model_test.cpu() \n",
    "X_test = torch_X_test\n",
    "y_test = torch_y_test.cpu()\n",
    "\n",
    "model_test.eval()  \n",
    "with torch.no_grad():\n",
    "    predY_test = model_test(X_test[:1024, :, :, :]).cpu()\n",
    "    \n",
    "pred = (predY_test > 0.5).float()\n",
    "\n",
    "error = nn.BCELoss()\n",
    "loss = error(pred, y_test[:1024])\n",
    "\n",
    "print('Loss (test data):', loss.item())\n",
    "\n",
    "print('Loss (test data): ' + str(loss.item()))\n",
    "print('---------------------------------------------------------------------------------------------------------------------------')\n",
    "# Compute predictions and accuracy\n",
    "accuracy_Y = (pred[:1024,0] == y_test[:1024,0]).float().mean()\n",
    "accuracy_E = (pred[:1024,1] == y_test[:1024,1]).float().mean()\n",
    "accuracy_S = (pred[:1024,2] == y_test[:1024,2]).float().mean()\n",
    "\n",
    "print(f'Accuracy of Arched_Eyebrows (test data): {accuracy_Y.item()*100}%')\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(predY_test[:,1].cpu(), y_test[:1024,0].cpu(), 'x', alpha=0.2, label='Young')\n",
    "plt.ylabel('True')\n",
    "plt.xlabel('Pred')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f'Accuracy of Attractive (test data): {accuracy_E.item()*100}%')\n",
    "\n",
    "# Plot for 'Eyeglasses'\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(predY_test[:,2].cpu(), y_test[:1024,1].cpu(), 'o', alpha=0.2, label='Eyeglasses')\n",
    "plt.ylabel('True')\n",
    "plt.xlabel('Pred')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f'Accuracy of Bags_Under_Eyes (test data): {accuracy_S.item()*100}%')\n",
    "\n",
    "# Plot for 'Smiling'\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(predY_test[:,3].cpu(), y_test[:1024,2].cpu(), 's', alpha=0.2, label='Smiling')\n",
    "plt.ylabel('True')\n",
    "plt.xlabel('Pred')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfd18f5-d843-4613-965d-710dfa0b8774",
   "metadata": {},
   "source": [
    "#                  GradCam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce291ea-9e94-41f5-b4a6-968f6e8e5521",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_module = os.path.join(os.getcwd(), 'pytorch-grad-cam')\n",
    "sys.path.append(path_to_module)\n",
    "\n",
    "\n",
    "from pytorch_grad_cam.grad_cam import GradCAM\n",
    "from pytorch_grad_cam.hirescam import HiResCAM\n",
    "from pytorch_grad_cam.grad_cam_elementwise import GradCAMElementWise\n",
    "from pytorch_grad_cam.ablation_layer import AblationLayer, AblationLayerVit, AblationLayerFasterRCNN\n",
    "from pytorch_grad_cam.ablation_cam import AblationCAM\n",
    "from pytorch_grad_cam.xgrad_cam import XGradCAM\n",
    "from pytorch_grad_cam.grad_cam_plusplus import GradCAMPlusPlus\n",
    "from pytorch_grad_cam.score_cam import ScoreCAM\n",
    "from pytorch_grad_cam.layer_cam import LayerCAM\n",
    "from pytorch_grad_cam.eigen_cam import EigenCAM\n",
    "from pytorch_grad_cam.eigen_grad_cam import EigenGradCAM\n",
    "from pytorch_grad_cam.random_cam import RandomCAM\n",
    "from pytorch_grad_cam.fullgrad_cam import FullGrad\n",
    "from pytorch_grad_cam.guided_backprop import GuidedBackpropReLUModel\n",
    "from pytorch_grad_cam.activations_and_gradients import ActivationsAndGradients\n",
    "from pytorch_grad_cam.feature_factorization.deep_feature_factorization import DeepFeatureFactorization, run_dff_on_image\n",
    "import pytorch_grad_cam.utils.model_targets\n",
    "import pytorch_grad_cam.utils.reshape_transforms\n",
    "import pytorch_grad_cam.metrics.cam_mult_image\n",
    "import pytorch_grad_cam.metrics.road\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.base_cam import BaseCAM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed023934-b4eb-4e09-84f3-805104a188af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradCAM(BaseCAM):\n",
    "    def __init__(self, model, target_layers, reshape_transform=None):\n",
    "        super(GradCAM, self).__init__(model, target_layers, reshape_transform)\n",
    "\n",
    "    def get_cam_weights(self, input_tensor, target_layer, target_category, activations, grads):\n",
    "        # 2D image\n",
    "        if len(grads.shape) == 4:\n",
    "            return np.mean(grads, axis=(2, 3))\n",
    "        \n",
    "        # 3D image\n",
    "        elif len(grads.shape) == 5:\n",
    "            return np.mean(grads, axis=(2, 3, 4))\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"Invalid grads shape. Shape of grads should be 4 (2D image) or 5 (3D image).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec17666f-8bf2-4d01-93cf-7986efd338ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_test.eval()\n",
    "target_layers = [model_test.resnet_model.layer4[-1]]  # Accessing the correct layer\n",
    "\n",
    "# Initialize Grad-CAM\n",
    "cam = GradCAM(model=model_test, target_layers=target_layers)\n",
    "\n",
    "# Transform for input image\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Function to visualize Grad-CAM\n",
    "def visualize_gradcam(example_image, target_category):\n",
    "    example_image = transform(example_image).unsqueeze(0).to(DEVICE).float()  # Ensure using float32\n",
    "\n",
    "    # Generate CAM\n",
    "    grayscale_cam = cam(input_tensor=example_image, targets=[ClassifierOutputTarget(target_category)])[0, :]\n",
    "\n",
    "    # Convert image to numpy for visualization\n",
    "    example_image_np = example_image.squeeze().cpu().numpy().transpose((1, 2, 0))\n",
    "    example_image_np = example_image_np * [0.229, 0.224, 0.225] + [0.485, 0.456, 0.406]\n",
    "    example_image_np = np.clip(example_image_np, 0, 1)\n",
    "\n",
    "    # Visualize CAM\n",
    "    cam_image = show_cam_on_image(example_image_np, grayscale_cam, use_rgb=True)\n",
    "    plt.imshow(cam_image)\n",
    "    plt.title(f'Grad-CAM for class {labels[target_category]}')\n",
    "    plt.show()\n",
    "\n",
    "# Example images for visualization\n",
    "for j in random.sample(range(X_train.shape[0]), 20):\n",
    "    for i, label in enumerate(labels):\n",
    "        print(f'Visualizing Grad-CAM for class: {label}')\n",
    "        visualize_gradcam(X_train[j], i)  # Visualize for each class (0, 1, 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
